⚠️ This paper was generated by Claude and has not yet been reviewed for accuracy. 



# Agentic Software Development at Scale: A Case Study in Documentation-Driven LLM Collaboration

**A White Paper on Building Complex Systems Through Human-AI Partnership**

*December 2025*

---

## Abstract

This paper presents a methodology for developing large-scale software systems through structured collaboration between human developers and large language models (LLMs). Using the Daemons Engine project—a 50,000+ line Python codebase comprising 17 distinct system phases—as a primary case study, we demonstrate how documentation-first development practices can enable effective "agentic" coding at scales previously considered impractical for AI-assisted development.

Key findings include: (1) focused, hierarchical documentation dramatically improves LLM output consistency across extended development cycles; (2) modular architecture patterns reduce context requirements and improve code quality; (3) human expertise remains essential for design decisions, architecture, and cross-system coordination; (4) the economics of token-based AI services create new cost structures for independent developers.

---

## 1. Introduction

### 1.1 The Promise and Limits of AI-Assisted Development

Large language models have demonstrated remarkable capability in code generation tasks. Models like GPT-4, Claude, and Codex can produce functional code from natural language descriptions, complete partial implementations, and explain existing codebases. However, practical application at production scale reveals significant limitations:

- **Context window constraints**: Even models with 100K+ token windows cannot hold entire large codebases simultaneously
- **Consistency drift**: Without explicit guidance, LLMs may introduce contradictory patterns across sessions
- **Architecture blindness**: LLMs excel at local code generation but struggle with system-wide design coherence
- **Hallucination risk**: Novel requirements may trigger plausible-sounding but incorrect implementations

These limitations have led to a common perception that AI-assisted development is suitable primarily for small tasks, prototypes, or isolated functions—not for building comprehensive production systems.

### 1.2 Research Questions

This paper addresses three questions:

1. Can structured documentation enable effective LLM-assisted development of complex, multi-system software?
2. What development practices maximize the quality and consistency of AI-generated code?
3. What role does human expertise play in an "agentic" development workflow?

### 1.3 Case Study: Daemons Engine

Daemons Engine is a real-time multiplayer game engine implemented in Python, featuring:

- **17 development phases** spanning core infrastructure through advanced gameplay systems
- **400+ automated tests** with continuous integration
- **80+ YAML content files** across 15 content types
- **Modular architecture** with 25+ distinct systems
- **WebSocket protocol** with JWT authentication
- **RESTful admin API** with 50+ endpoints

The project was developed over approximately 10 months through iterative human-LLM collaboration, with the human contributor providing architecture, design, and project management while the LLM generated implementation code.

---

## 2. Methodology: Documentation-Driven Development

### 2.1 The Documentation Hierarchy

The core insight enabling large-scale agentic development is treating documentation as a **context management system** for the LLM. Rather than expecting the AI to infer patterns from code, we provide explicit guidance at multiple abstraction levels:

```
Level 1: Architecture Documents
├── ARCHITECTURE.md (system overview, core concepts)
├── protocol.md (client-server communication)
└── roadmap.md (phase definitions, completion criteria)

Level 2: Phase-Specific Designs
├── build_docs/PHASE_X_DESIGN.md
├── build_docs/PHASE_X_IMPLEMENTATION.md
└── build_docs/PHASE_X_TESTING.md

Level 3: Schema Documentation
├── SCHEMAS_FOR_DEVS.md (database tables, constraints)
├── SCHEMAS_FOR_CONTENT_CREATORS.md (YAML formats)
└── world_data/*/_schema.yaml (per-content-type schemas)

Level 4: Inline Documentation
├── Docstrings (function/class level)
├── Type hints (structural contracts)
└── Comments (implementation rationale)
```

### 2.2 Context Window Management

A critical challenge in agentic development is managing what information the LLM receives during each session. Our approach uses **focused context files**—curated subsets of documentation optimized for specific tasks:

**Example: Implementing a New Game System**

```
Context provided:
1. ARCHITECTURE.md (10% - relevant sections only)
2. Existing system interfaces (event bus, persistence patterns)
3. Phase-specific design document (full)
4. Related schema definitions
5. Test fixture patterns from similar systems

Context excluded:
- Unrelated system implementations
- Historical design discussions
- Client-side code
- Administrative tooling
```

This selective context loading typically reduces token requirements by 70-80% while improving output relevance.

### 2.3 The Development Cycle

Each feature or system follows a structured cycle:

```
┌─────────────────────────────────────────────────────────────┐
│  1. DESIGN PHASE (Human-led, AI-assisted)                   │
│     - Define requirements and acceptance criteria           │
│     - Establish interfaces with existing systems            │
│     - Document design decisions and rationale               │
│     - Create implementation plan                            │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│  2. IMPLEMENTATION PHASE (AI-led, Human-reviewed)           │
│     - Generate code from design documents                   │
│     - Iterate on human feedback                             │
│     - Write tests alongside implementation                  │
│     - Update documentation with implementation details      │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│  3. INTEGRATION PHASE (Collaborative)                       │
│     - Verify cross-system interactions                      │
│     - Resolve conflicts with existing code                  │
│     - Performance testing and optimization                  │
│     - Documentation reconciliation                          │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│  4. DOCUMENTATION PHASE (Human-verified)                    │
│     - Update architecture documents                         │
│     - Create focused context files for future phases        │
│     - Archive design rationale                              │
│     - Update schema documentation                           │
└─────────────────────────────────────────────────────────────┘
```

### 2.4 Pattern Establishment and Enforcement

A key success factor is establishing clear patterns early and enforcing them consistently. When the LLM encounters a new requirement, it can reference existing patterns rather than inventing new approaches:

**Example: GameContext Pattern**

```python
# Established pattern: Systems receive shared context
class GameContext:
    """Shared context passed to all game systems."""
    world: World
    combat_system: CombatSystem
    effect_system: EffectSystem
    event_dispatcher: EventDispatcher
    # ... additional systems

# New system follows pattern
class WeatherSystem:
    def __init__(self, context: GameContext):
        self.context = context
        self.world = context.world
    
    async def update_weather(self, area_id: str):
        # Can access other systems through context
        await self.context.event_dispatcher.dispatch(...)
```

By documenting this pattern explicitly, subsequent systems follow it automatically without repeated explanation.

---

## 3. Architectural Patterns for Agentic Development

### 3.1 Modular System Design

The Daemons Engine architecture deliberately isolates systems to minimize cross-cutting concerns:

```
┌──────────────────────────────────────────────────────────────────┐
│                        WorldEngine                                │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │
│  │   Combat    │  │   Quest     │  │  Weather    │   ...        │
│  │   System    │  │   System    │  │   System    │              │
│  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘              │
│         │                │                │                      │
│         └────────────────┼────────────────┘                      │
│                          ▼                                       │
│                  ┌───────────────┐                               │
│                  │ EventDispatcher│                               │
│                  └───────────────┘                               │
│                          │                                       │
│         ┌────────────────┼────────────────┐                      │
│         ▼                ▼                ▼                      │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │
│  │  Persistence│  │    Time     │  │   State     │              │
│  │    Layer    │  │   Manager   │  │   Tracker   │              │
│  └─────────────┘  └─────────────┘  └─────────────┘              │
└──────────────────────────────────────────────────────────────────┘
```

**Benefits for agentic development:**
- Each system can be implemented in isolation with focused context
- System interfaces serve as natural documentation boundaries
- Cross-system bugs are easier to isolate and diagnose
- New systems can be added without understanding entire codebase

### 3.2 Event-Driven Communication

Systems communicate through a central event dispatcher rather than direct method calls:

```python
# System A emits event
await event_dispatcher.dispatch(Event(
    type="combat_damage",
    payload={"target_id": npc_id, "damage": 25}
))

# System B subscribes to event type
@event_dispatcher.subscribe("combat_damage")
async def on_combat_damage(event: Event):
    # Update quest progress for "kill X enemies" objectives
    await quest_system.check_kill_objective(event.payload)
```

**Benefits:**
- Systems don't need direct references to each other
- New systems can subscribe to existing events without modification
- Event contracts are explicit and documentable
- Testing is simplified—events can be mocked independently

### 3.3 Data-Driven Content

Game content is defined in YAML files with explicit schemas:

```yaml
# world_data/npcs/goblin_scout.yaml
id: goblin_scout
name: Goblin Scout
description: A small, wiry goblin with beady eyes.
npc_type: hostile
level: 2
max_health: 20
behavior:
  aggressive: true
  wander: true
  flee_health_percent: 20
loot_table:
  - item_id: rusty_dagger
    drop_chance: 0.4
```

**Benefits for agentic development:**
- Content generation is separable from code generation
- LLMs excel at structured data generation with explicit schemas
- Content can be validated independently of runtime behavior
- Non-programmers can contribute content

### 3.4 Type-First Development

Comprehensive type hints serve as machine-readable documentation:

```python
from dataclasses import dataclass
from typing import Dict, List, Optional

@dataclass
class WorldPlayer:
    id: str
    name: str
    room_id: str
    level: int = 1
    current_health: int = 100
    max_health: int = 100
    active_effects: Dict[str, Effect] = field(default_factory=dict)
    inventory: List[str] = field(default_factory=list)
    equipped_items: Dict[str, str] = field(default_factory=dict)
    
    def get_effective_stat(self, stat_name: str) -> int:
        """Calculate stat value including effect modifiers."""
        base = getattr(self, f"base_{stat_name}", 10)
        modifier = sum(
            effect.stat_modifiers.get(stat_name, 0)
            for effect in self.active_effects.values()
        )
        return base + modifier
```

**Benefits:**
- LLMs can infer correct usage from type signatures
- IDE tooling catches type mismatches before runtime
- Type definitions serve as implicit documentation
- Refactoring is safer with explicit contracts

---

## 4. Quality Assurance in Agentic Development

### 4.1 Testing Strategy

AI-generated code requires rigorous testing. The Daemons Engine test suite follows a tiered approach:

```
Test Pyramid:
                    ╱╲
                   ╱  ╲
                  ╱ E2E╲           (16 tests)
                 ╱──────╲
                ╱ Integ. ╲         (57 tests)
               ╱──────────╲
              ╱   System   ╲       (89 tests)
             ╱──────────────╲
            ╱     Unit       ╲     (175+ tests)
           ╱──────────────────╲
```

**Unit tests** verify individual functions and classes in isolation:

```python
def test_player_takes_damage():
    player = WorldPlayer(id="test", name="Test", room_id="room1")
    player.current_health = 100
    
    player.take_damage(30)
    
    assert player.current_health == 70
    assert player.is_alive()
```

**System tests** verify individual systems with mocked dependencies:

```python
@pytest.mark.asyncio
async def test_weather_transition():
    weather_system = WeatherSystem(mock_context)
    
    await weather_system.set_weather("area1", WeatherType.RAIN)
    
    assert weather_system.get_current_weather("area1").type == WeatherType.RAIN
```

**Integration tests** verify cross-system interactions:

```python
@pytest.mark.asyncio
async def test_combat_triggers_quest_progress():
    # Setup: Player has active "kill 5 goblins" quest
    # Action: Player kills goblin via combat system
    # Verify: Quest system registers kill progress
```

### 4.2 Code Review Process

All AI-generated code undergoes human review with specific focus areas:

1. **Architectural consistency**: Does the code follow established patterns?
2. **Edge case handling**: Are error conditions properly managed?
3. **Performance implications**: Are there obvious inefficiencies?
4. **Security considerations**: Are inputs validated? Are queries parameterized?
5. **Documentation accuracy**: Do comments match implementation?

### 4.3 Continuous Integration

The project uses GitHub Actions for automated quality gates:

```yaml
# .github/workflows/test.yml
name: Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11", "3.12", "3.13"]
    
    steps:
      - uses: actions/checkout@v3
      - name: Run tests
        run: pytest --cov=app --cov-report=xml
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

### 4.4 Security Considerations

AI-generated code may introduce security vulnerabilities through:
- Incomplete input validation
- Insecure default configurations
- Improper error handling revealing sensitive information
- SQL injection (mitigated by ORM usage)

The Phase 16 security audit addressed these concerns systematically:

| Security Control | Implementation |
|-----------------|----------------|
| Input sanitization | `InputSanitizer` class validates all command inputs |
| Rate limiting | `slowapi` for HTTP, custom limiter for WebSocket |
| JWT hardening | Issuer validation, token binding, secure key requirements |
| Account lockout | Failed login tracking with automatic lockout |
| Audit logging | `SecurityEvent` table records all auth events |

---

## 5. Economic Considerations

### 5.1 Token Costs

Agentic development introduces direct costs absent from traditional development:

| Phase | Estimated Tokens | Approximate Cost* |
|-------|-----------------|-------------------|
| Core systems (0-4) | 2-3M | $30-50 |
| Mid-tier features (5-9) | 3-5M | $50-80 |
| Advanced systems (10-17) | 5-8M | $80-120 |
| Debugging/iteration | 2-4M | $30-60 |
| **Total** | **12-20M** | **$190-310** |

*Costs estimated based on 2024 Claude API pricing; actual costs vary by provider and tier.

### 5.2 Time Investment

Human time investment follows a different distribution than traditional development:

| Activity | Traditional Dev | Agentic Dev |
|----------|----------------|-------------|
| Design & architecture | 20% | 35% |
| Implementation | 50% | 15% |
| Code review | 10% | 25% |
| Testing | 15% | 20% |
| Documentation | 5% | 5% |

The shift from implementation to design/review reflects the changed role of the human developer.

### 5.3 Cost-Benefit Analysis

For this project scale (50K+ LOC), agentic development provided:

**Benefits:**
- Reduced implementation time by estimated 60-70%
- Consistent code style across entire codebase
- Comprehensive documentation as a development byproduct
- Test coverage from the beginning (tests written alongside features)

**Costs:**
- Direct token expenditure (~$250 total)
- Increased design documentation time
- Learning curve for effective prompting
- Review overhead for AI-generated code

### 5.4 Accessibility Implications

Token-based pricing creates new barriers and opportunities:

**Barriers:**
- Effective agentic development requires financial investment
- Students and hobbyists may be priced out of advanced AI assistance
- Quality of development increasingly correlates with available budget

**Opportunities:**
- Open-source projects can share AI-assisted results freely
- Documentation created for AI context benefits all contributors
- Lower implementation barrier enables more ambitious projects

---

## 6. Limitations and Failure Modes

### 6.1 Observed Failure Patterns

Despite careful methodology, several failure modes were observed:

**Pattern drift over sessions:**
```
Session 1: EventDispatcher uses async/await
Session 5: New system uses synchronous event emission
Resolution: Explicit pattern documentation + review checklist
```

**Over-engineering:**
```
Request: "Add a simple health regeneration system"
Result: Abstract base classes, multiple inheritance, plugin architecture
Resolution: Constrain scope in prompts; "keep it simple" explicit instruction
```

**Hallucinated APIs:**
```
Generated: await db.execute_async(query)  # Doesn't exist
Actual: await db.execute(query)
Resolution: Include library documentation in context
```

**Incomplete error handling:**
```
Generated: result = await some_operation()  # Assumes success
Should be: try/except with specific error handling
Resolution: Error handling as explicit review checklist item
```

### 6.2 Tasks Poorly Suited for LLM Generation

Certain development tasks remained primarily human-driven:

- **Performance optimization**: LLMs generate correct but not necessarily efficient code
- **Debugging complex state**: Multi-system interaction bugs require human reasoning
- **Security architecture**: High-stakes decisions require human accountability
- **User experience design**: Subjective quality judgments beyond AI capability
- **Novel algorithm design**: Truly new approaches vs. recombination of known patterns

### 6.3 Documentation Maintenance Burden

The documentation-first methodology creates ongoing maintenance requirements:

- Design documents must be updated when implementations diverge
- Focused context files require curation as systems evolve
- Schema documentation must stay synchronized with actual schemas
- Architecture documents can become stale across major refactors

---

## 7. Recommendations for Practitioners

### 7.1 Getting Started

For developers beginning agentic development:

1. **Start small**: Build confidence with isolated features before full systems
2. **Establish patterns early**: First implementations become templates for later code
3. **Document decisions**: Record *why* choices were made, not just what was done
4. **Test immediately**: Write tests alongside generated code, not after
5. **Review everything**: AI code requires the same scrutiny as junior developer code

### 7.2 Scaling Up

For larger projects:

1. **Invest in architecture documentation**: The time pays dividends across all phases
2. **Create focused context files**: Curate subsets for specific task types
3. **Establish review checklists**: Systematic review catches common AI mistakes
4. **Plan for iteration**: First-pass AI code often needs refinement
5. **Budget for tokens**: Include AI costs in project planning

### 7.3 Team Collaboration

For teams using agentic development:

1. **Standardize prompting patterns**: Consistent approaches yield consistent results
2. **Share context files**: Curated documentation benefits all team members
3. **Centralize pattern documentation**: Single source of truth for coding standards
4. **Track AI usage**: Monitor which tasks benefit most from AI assistance
5. **Maintain human expertise**: AI assistance supplements, doesn't replace, skill development

---

## 8. Future Directions

### 8.1 Evolving AI Capabilities

As LLM capabilities improve, the methodology will likely evolve:

- **Larger context windows**: Reduce need for focused context curation
- **Better memory**: Maintain consistency across sessions without explicit documentation
- **Multi-modal understanding**: Diagram-to-code, UI mock-up implementation
- **Autonomous agents**: Longer-running tasks with less human intervention

### 8.2 Tooling Improvements

Supporting tools are emerging:

- **Context management**: Automated curation of relevant documentation
- **Pattern enforcement**: Static analysis for AI-generated code
- **Cost tracking**: Token usage analytics and optimization
- **Review assistance**: AI-assisted review of AI-generated code

### 8.3 Process Maturation

As the field matures, expect:

- **Best practices standardization**: Industry consensus on effective patterns
- **Quality metrics**: Measurable indicators of AI code quality
- **Risk frameworks**: Formal assessment of AI-generated code risks
- **Certification programs**: Validation of agentic development competency

---

## 9. Conclusion

This case study demonstrates that structured documentation and careful methodology can enable effective LLM-assisted development of complex software systems. The Daemons Engine project—with its 17 phases, 25+ systems, and 400+ tests—provides evidence that "agentic" development at scale is not merely possible but potentially efficient.

Key success factors include:
- **Hierarchical documentation** serving as both human reference and AI context
- **Modular architecture** enabling focused, isolated development
- **Pattern establishment** providing templates for consistent code generation
- **Rigorous testing** catching AI errors before production
- **Human expertise** guiding architecture, design, and quality assurance

The methodology does not eliminate the need for skilled developers. Rather, it shifts the human role from implementation to architecture, review, and coordination. The developer becomes a technical lead for an AI "team member" that excels at implementation but requires guidance on system design and quality standards.

For organizations and individuals considering agentic development, we recommend starting with well-scoped projects, investing heavily in documentation infrastructure, and maintaining healthy skepticism of AI-generated code. The technology is powerful but not autonomous—human judgment remains essential.

---

## References

1. Chen, M., et al. (2021). "Evaluating Large Language Models Trained on Code." arXiv:2107.03374
2. Austin, J., et al. (2021). "Program Synthesis with Large Language Models." arXiv:2108.07732
3. Daemons Engine Project Documentation. https://github.com/[repository]
4. FastAPI Documentation. https://fastapi.tiangolo.com/
5. SQLAlchemy 2.0 Documentation. https://docs.sqlalchemy.org/

---

## Appendix A: Project Statistics

| Metric | Value |
|--------|-------|
| Total lines of code | ~50,000 |
| Python source files | 120+ |
| Test files | 45+ |
| Automated tests | 400+ |
| YAML content files | 80+ |
| Documentation files | 25+ |
| Development phases | 17 |
| Database tables | 25+ |
| REST API endpoints | 50+ |
| WebSocket message types | 15+ |

## Appendix B: Technology Stack

| Layer | Technology |
|-------|------------|
| Language | Python 3.11+ |
| Web framework | FastAPI |
| ASGI server | Uvicorn |
| Database ORM | SQLAlchemy (async) |
| Database | SQLite |
| Migrations | Alembic |
| Authentication | python-jose (JWT), passlib (Argon2) |
| Testing | pytest, pytest-asyncio |
| CI/CD | GitHub Actions |
| Code quality | ruff, black, isort |
| Documentation | Markdown |

---

*This white paper represents the methodological findings from developing the Daemons Engine project. The project and documentation are available under MIT license.*

⚠️ This paper has not yet been reviewed for accuracy